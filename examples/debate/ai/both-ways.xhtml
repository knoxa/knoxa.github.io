<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml"
	xmlns:dc="http://purl.org/dc/elements/1.1/" 
	xmlns:rdfs="http://www.w3.org/2000/01/rdf-schema#"
	xmlns:skos="http://www.w3.org/2004/02/skos/core#"
	xmlns:time="https://www.w3.org/TR/owl-time/"
	xmlns:ies="http://ies.data.gov.uk/ies4#"
	xmlns:core="https://knoxa.github.io/ai#"
	xmlns:bostrom="https://search.worldcat.org/search?q=isbn%3A9780199678112#"
	xmlns:this="https://doi.org/10.1111/rati.12320#"
	>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
<link type="text/css" href="master.css" rel="stylesheet"/>
<title>Müller and Cannon</title>
<link rel="schema.DC" href="http://purl.org/dc/elements/1.1/"/>
<meta name="DC.source" content="The Elephant and the Fly"/>
<meta name="DC.identifier" content="Example ..."/>
</head>
<body>

<h2>Existential risk from AI and orthogonality: Can we have it both ways?</h2>

<article>

	<div about="https://doi.org/10.1111/rati.12320">
	<p class="ref"><a href="https://doi.org/10.1111/rati.12320">Müller, V. C., Cannon, M. (2022)</a></p>

	<blockquote>
		<span class="ellipsis">[The reconstruction shows that]</span> the argument for existential risk from AI has two premises, 
		namely <span class="claim" about="core:singularity">the singularity claim</span> and <span class="claim" about="bostrom:orthogonality">the orthogonality thesis</span>.
	</blockquote>
	
	<div class="analysis">
		<ul>
			<li class="premise">
				Taken together, the <span class="premise refer" about="core:singularity">singularity claim</span> 
				and <span class="premise refer" about="bostrom:orthogonality">orthogonality thesis</span>
				<span class="support" about="bostrom:supportextinction">may amount to</span> 
				<span class="conclusion" about="core:extinction">an existential threat from AI.</span>
			</li> 
		</ul>
	</div>
	
	<blockquote>
		We find that the singularity claim requires a notion of ‘general intelligence’, while the orthogonality thesis requires a notion of ‘instrumental intelligence’.
	</blockquote>
	
	<blockquote class="premise" about="this:concern">
		Our concern is with the validity of the argument: It appears that for each of the two premises to be charitably interpreted as true, 
		they must be interpreted as using the term ‘intelligence’ in different ways. <span class="conflict">If that is the case, they cannot be combined as premises</span> into a valid 
		argument for <span class="undercut" about="bostrom:supportextinction">existential risk from AI</span>. 
	</blockquote>
	
	</div>
	
</article>


<h3>Argument map</h3>
<p>
<a href="AIF/both-ways.txt">
<img class="medium" src="images/both-ways.svg" alt="argument map for 'both ways' claim" />
</a>
</p>

<article>
	
	<p class="note">The authors explore a bit further ...</p>
	
	<p>
		Is there a notion of intelligence that we can use in both premises and with which we can interpret both premises as true?
	</p>
	
	<p>
		If the AI is capable of realising what is relevant, why would the realisations of the AI stop before it realises the 
		relevance of reflecting on goals?
	</p>
	
		
	<p class="note">... which we should come back to later.</p>
	
</article>

</body>
</html>