<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml"
	xmlns:dc="http://purl.org/dc/elements/1.1/" 
	xmlns:rdfs="http://www.w3.org/2000/01/rdf-schema#"
	xmlns:skos="http://www.w3.org/2004/02/skos/core#"
	xmlns:time="https://www.w3.org/TR/owl-time/"
	xmlns:ies="http://ies.data.gov.uk/ies4#"
	xmlns:core="https://knoxa.github.io/ai/core#"
	xmlns:this="https://knoxa.github.io/ai/singularity#"
	>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
<link type="text/css" href="master.css" rel="stylesheet"/>
<title>AI Singularity</title>
<link rel="schema.DC" href="http://purl.org/dc/elements/1.1/"/>
<meta name="DC.source" content="The Elephant and the Fly"/>
<meta name="DC.identifier" content="Example ..."/>
</head>
<body>

<h1>The singularity claim</h1>

<p class="note">This idea traces back to Good's description of an ultraintelligent machine creating an 'intelligence explosion' and Vinge's coining of the term 'technological singularity'.</p>

<article>

	<h2>Speculations Concerning the First Ultraintelligent Machine</h2>

	<div about="https://doi.org/10.1016/S0065-2458%2808%2960418-0">
	
	<p class="ref"><a href="https://www.stat.vt.edu/content/dam/stat_vt_edu/graphics-and-pdfs/research-papers/Technical_Reports/TechReport05-3.pdf">Good, I. J. (1965)</a>:</p>

		<blockquote about="this:good1">
		Let an ultraintelligent machine be defined as a machine that can far surpass all the intellectual activities of any man however clever. 
		Since the design of machines is one of these intellectual activities, an ultraintelligent machine could design even better machines; 
		there would then unquestionably be an ‘intelligence explosion’, and the intelligence of man would be left far behind. 
		</blockquote>
		
		<blockquote about="this:good2">
		Thus the first ultraintelligent machine is the last invention that man need ever make, provided that the machine is docile enough to tell us 
		how to keep it under control.
		</blockquote>		
		
		<div class="analysis">
			<ul>
				<li class="premise aside">
					If <span class="premise refer" about="this:good2">it</span> is not 'docile enough',  <span class="support">the creation of the 'first ultraintelligent machine'</span> is <span class="conclusion refer" about="core:singularity">the singularity</span>.
				</li>
			</ul>
		</div>
				
	</div>
	
</article>

<article>
	<h2>The Coming Technological Singularity</h2>

	<div about="https://mindstalk.net/vinge/vinge-sing.html">
	
		<p class="ref"><a href="https://mindstalk.net/vinge/vinge-sing.html">Vinge, V. (1993)</a></p>

		<blockquote>
			When greater-than-human intelligence drives progress, that progress will be much more rapid. In fact, there seems no reason why progress itself 
			would not involve the creation of still more intelligent entities - on a still-shorter time scale. 
		</blockquote>
		
		<blockquote>
			From the human point of view this change will be a throwing away of all the previous rules, perhaps in the blink of an eye, an exponential 
			runaway beyond any hope of control. 
		</blockquote>
		
		<div class="analysis">
			<ul>
				<li about="core:singularity" class="claim">
					This is <em>the singularity claim</em>: Superintelligent AI is a realistic prospect and it would be out of human control.
				</li>
			</ul>
		</div>
	</div>
</article>

<h3>Argument map</h3>
<p>
<a href="AIF/singularity.txt">
<img class="medium" src="images/singularity.svg" alt="argument map for the singularity claim" />
</a>
</p>

</body>
</html>