# Arguing about Artificial Intelligence

## Introduction
This is an exercise in capturing debate, on a suitably large and interesting topic, to demonstrate (and further develop) ideas about collecting, evaluating and explaining arguments. It's [argumentation](https://dstl.github.io/eleatics/doc/argumentation/). This document provides a running commentary on the debate. I'll discuss the [technical details](technical.md) separately. 

## Existential threat
Let's start with the [singularity claim](singularity.xhtml) workbook. This grounds *the singularity claim* in essays by Good and Vinge. Next, Bostrom defines a [superintelligence](superintelligence.xhtml#superintelligence) and a case for it potentially threatening humanity. So far, there is no contention.

[MÃ¼ller and Cannon](both-ways.xhtml) reconstruct the existential risk argument and attack its validity. This is an undercut, which is expressed in AIF by attacking an RA-node. Something to think about here is that the restated existential threat argument isn't exactly the one expressed by Bostrom: It uses *the singularity claim* as a premise instead of a singleton, and it doesn't use the *instrumental convergence thesis*. This doesn't affect the validity of the undercutting argument, but it does mean that there are two RA-nodes that conclude *humanity quickly becomes extinct*, and only one of them is undercut:  

![Undercutting the extinction argument](images/extinction.svg)

A lesson here is that we need to consider whether two arguments that reach the same conclusion are really different, or whether one is some restatement (or weaker form) of the other. If the two arguments are different, undercutting one of them leaves the conclusion valid via the other route. We'll know the undercutter is there because it will force an extension in which it is acceptable - but that extension won't materially affect the conclusions you can draw from the argument map. There's a point to make that evaluating an argument map shouldn't just be a one-shot exercise to get 'the answer'; it should be an iterative process that helps you assess, develop and explore argument.

In this particular case, we need to understand the differences between 'general intelligence' and 'instrumental intelligence'. Another point to consider is whether 'the technical singularity' and 'a singleton' mean the same thing, in this context at least. At this point, I extented [superintelligence](superintelligence.xhtml#later) capture Bostrom distancing himself slightly from 'the singularity'. Later, I may decide to make 'the technical singularity' and 'a singleton' equivalent, and think about how to model this in AIF with a MA-node.

## The singularity
The argument for existential threat depends on the singularity claim, which in turn builds on the premise of an "intelligence explosion" over a short time scale, "perhaps in the blink of an eye". In his book [The Singularity is Near](https://en.wikipedia.org/wiki/The_Singularity_Is_Near), Ray Kurzweil develops [the law of accelerating returns](accelerating-returns.xhtml) in support of this premise. Some arguments against the singularity, such as [the complexity brake](complexity-brake.xhtml), question the exponential nature of the intelligence explosion.

David Chalmers published [a philosophical analysis of the singularity](chalmers.xhtml) that constructs and discusses the argument for the singularity and potential defeaters. This provides a useful framework for assessing arguments for and against. It too raises questions about what is meant by 'intelligence'.

Chalmers considers three classes of artificial intelligence: AI, AI+ and AI++. The first is human-level intelligence (meaning Artificial General Intelligence, AGI), the second is greater than human intelligence (of the sort that triggers the "intelligence explosion"), and the third is the singularity (or superintelligence). If we're joining argument from different sources together, we need to understand variations in terminology. Mapping terminology is something that would both help argument mining and be helped by argument mining.

*work-in-progress*